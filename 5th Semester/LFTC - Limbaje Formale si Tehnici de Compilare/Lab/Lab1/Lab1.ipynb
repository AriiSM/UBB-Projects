{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44eebcbc",
   "metadata": {},
   "source": [
    "## Partea 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3adbd9-5965-4e90-9b4a-90051deb0846",
   "metadata": {},
   "source": [
    "**1. Specificarea Minilimbajului de Programare (MLP)**\n",
    "\n",
    "Limbajul trebuie să conțină cel puțin următoarele instrucțiuni și tipuri de date:\n",
    "\n",
    "**2 tipuri de date simple** și **un tip de date definit de utilizator**\n",
    "\n",
    "### Instructiuni:\n",
    "- **O instrucțiune de atribuire**\n",
    "- **O instrucțiune de intrare/ieșire**\n",
    "- **O instrucțiune de selecție (condițională)**\n",
    "- **O instrucțiune de ciclare**\n",
    "\n",
    "Pe lângă acestea, vor exista unele **restricții suplimentare** referitoare la identificatori și constante (vezi secțiunea 3.1).\n",
    "\n",
    "Se cere ca specificarea să respecte structurile sintactice ale limbajului ales, într-o formă (foarte) simplificată. De asemenea, aceasta va fi suficient de generală astfel încât să descrie constructiile limbajului folosite pentru scrierea programelor de la punctul 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a35454-7fc5-4fab-b4db-4656c4550fb9",
   "metadata": {},
   "source": [
    "\n",
    "    BNF pt acest lab:\n",
    "\n",
    "    <program> ::= <instructiuni_compuse>\n",
    "    <instructiuni_compuse> ::= <instructiune> | <instructiune> <instructiuni_compuse>\n",
    "    <instructiune> ::= <atribuire> | <instructiune_if> | <instructiune_while> | <instructiune_in> | <instructiune_out>\n",
    "\n",
    "    <atribuire> ::= ID = <expresie>\n",
    "    <expresie> ::= CONST_INT | CONST_FLOAT | CONST_CHAR | <structura> | ID | ID <operatie> <expresie>\n",
    "    <structura> ::= <lista_pereche_cheie_valoare>\n",
    "    <lista_pereche_cheie_valoare> ::=  <pereche_cheie_valoare> |  <pereche_cheie_valoare> , <lista_pereche_cheie_valoare> \n",
    "    <pereche_cheie_valoare> ::= ID : <expresie>\n",
    "\n",
    "    <operatie> ::= \"!=\" | \"==\" | \"<=\" | \">=\" | \"+\" | \"-\" | \"*\" | \"%\" | \"/\"\n",
    "     \n",
    "    <instructiune_if> ::= if (<conditie>): <instructiuni_compuse> |\n",
    "                          if (<conditie>): <instructiuni_compuse> else <instructiuni_compuse>\n",
    "\n",
    "    <instructiune_while> ::= while(<conditie>):<instructiuni_compuse>\n",
    "    <conditie> ::= ID <operatie> ID | ID <operatie> CONST\n",
    "\n",
    "    <instructiune_in> ::= ID = input() | ID = int(input())\n",
    "    <instructiune_out> ::= print(ID)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    BNF pt Lab4:\n",
    "\n",
    "    <program> ::= <tip> ID { <lista_instructiuni> \"return\" DECIMAL ; }\n",
    "    <lista_instructiuni> ::= <instructiune> | <instructiune> <lista_instructiuni>\n",
    "    <instructiune> ::= <declarare> | <atribuire> | <intrare> | <iesire> | <conditional> | <ciclare>\n",
    "\n",
    "    <declarare> ::= <tip> ID = <expresie> ; | <tip> ID ;\n",
    "    <tip> ::= \"int\" | \"float\"\n",
    "\n",
    "    <atribuire> ::= ID = <expresie> ;\n",
    "    <expresie> ::= <atom> | <expresie> <op> <atom>\n",
    "    <op> ::= \"+\" | \"-\" | \"*\" | \"/\" | \"==\"\n",
    "\n",
    "    <atom> ::= ID | <constant>\n",
    "    <constant> ::= CONST_INT | CONST_FLOAT \n",
    "\n",
    "    <intrare> ::= \"cin\" \">>\" ID \";\"\n",
    "    <iesire> ::= \"cout\" \"<<\" <expresie> \";\"\n",
    "\n",
    "    <conditional> ::= if (<conditie>) {<lista_instructiuni>} <altfel>\n",
    "    <altfel> ::= else {<lista_instructiuni>} | nimic\n",
    "    \n",
    "    <conditie> ::= <expresie> <relatie> <expresie>\n",
    "    <relatie> ::= \"<\" | \">\" | \"!=\" | \"==\" | \"<=\" | \">=\" \n",
    "\n",
    "    <ciclare> ::= <while_loop>\n",
    "    <while_loop> ::= while (<expresie>) {<lista_instructiuni>}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d937a3-2380-4483-a41a-cb6686ca8651",
   "metadata": {},
   "source": [
    "**Mini-Limbaj de Programare (MLP)**\n",
    "\n",
    "**Introducere**\n",
    "- Acest README conține trei mini-programe scrise în limbajul de programare specificat, care rezolvă următoarele probleme:\n",
    "1. Calculează perimetrul și aria cercului de o rază dată.\n",
    "2. Determină cel mai mare divizor comun (cmmdc) al două numere naturale.\n",
    "3. Calculează suma a n numere citite de la tastatură."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bbfd021-1f09-4815-826f-7621074b371d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3.14\u001b[39m\n\u001b[1;32m----> 2\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m perimetru \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m pi \u001b[38;5;241m*\u001b[39m r\n\u001b[0;32m      4\u001b[0m aria \u001b[38;5;241m=\u001b[39m pi \u001b[38;5;241m*\u001b[39m r \u001b[38;5;241m*\u001b[39m r\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "pi = 3.14\n",
    "r = int(input())\n",
    "perimetru = 2 * pi * r\n",
    "aria = pi * r * r\n",
    "print (perimetru)\n",
    "print (aria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06d17b08-d2a3-419c-a607-83ae00ee1f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "a = int(input()) #10\n",
    "b = int(input()) #5\n",
    "while(a != 0):\n",
    "    r = b%a\n",
    "    b = a\n",
    "    a = r\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69f7b99b-079a-433c-953a-54e713418cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "n = int(input()) #2\n",
    "i = 1\n",
    "s = 0\n",
    "while(i <= n):\n",
    "    s = s + i\n",
    "    i = i + 1\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74d8127-2a45-486c-91ee-79ca4b50f8dc",
   "metadata": {},
   "source": [
    "**Programe cu Erori în Mini-Limbaj de Programare (MLP)**\n",
    "\n",
    "**Introducere**\n",
    "\n",
    "Acest document conține două programe care conțin erori conform specificațiilor minilimbajului de programare (MLP) definit. \n",
    "\n",
    "1. **Primul program** conține două erori care sunt, de asemenea, erori în limbajul original.\n",
    "2. **Al doilea program** conține două erori conform MLP, dar care nu sunt erori în limbajul original.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eac56dd-757f-49fe-b8bf-2113ffd7aff2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2785721781.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[9], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    r = int(input()  #eroare de sintaxa -> r = int(input())\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "pi = 3.14\n",
    "r = int(input()  #eroare de sintaxa -> r = int(input())\n",
    "perimetru = 2 * pi * r\n",
    "aria = pi * r * r\n",
    "print perimetru) # eroare de sintaxa -> print (perimetru)\n",
    "print (aria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0d0b221-20e3-4c7d-b14a-17db59de43fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cmmdc: 5\n"
     ]
    }
   ],
   "source": [
    "a, b = map(int, input().split())  # noi am definit individual cu input urile sa fie pe randuri separate\n",
    "while(a != 0):\n",
    "    r = b%a\n",
    "    b = a\n",
    "    a = r\n",
    "print(\"Cmmdc:\", b)  #nu putem scrie mai mult in print in afara de ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760ef52",
   "metadata": {},
   "source": [
    "**Activitate in timpul laboratorului**\n",
    "- Specificati atributele:\n",
    "\n",
    "    'a += ?'\n",
    "\n",
    "    'a -= ?'\n",
    "\n",
    "    'a /= ?'\n",
    "\n",
    "    'a %= ?'\n",
    "\n",
    "    'si'\n",
    "\n",
    "    'a++'\n",
    "\n",
    "    '++a'\n",
    "\n",
    "    'a--'\n",
    "\n",
    "    '--a'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eadba7",
   "metadata": {},
   "source": [
    "**Rezultat:** \n",
    "    \n",
    "    <program> ::= <instructiuni>\n",
    "    <instructiuni> ::= ID <operator>= ID | ID <operator>= CONST | ID <operators><operators> | <operators><operators>ID\n",
    "    <operators> ::= \"+\" | \"-\" |\n",
    "    <operator> ::= <operators> | \"/\" | \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f55596",
   "metadata": {},
   "source": [
    "## Partea 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e499b6f",
   "metadata": {},
   "source": [
    "**Program ce identifica atomii lexicali (ce vor fi trimisi spre analiza sintactica) si prezentarea acestora (prin afisare pe ecran, scriere in fisier, etc.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b6b540d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parsare(filename):\n",
    "    # Citim codul pe care vrem sa-l impartim in atomi\n",
    "    with open(filename, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Cream un set in care vom pune atomii\n",
    "    atomSet = []\n",
    "    for line in lines:\n",
    "        words = line.strip().split(\" \")\n",
    "        for word in words:\n",
    "            atomSet.append(word)\n",
    "    atomSet.pop()\n",
    "\n",
    "    # Scriem atomii in fisier\n",
    "    with open(\"atomsPy.txt\", \"w\") as f:\n",
    "        for atom in atomSet:\n",
    "            f.write(atom + \"\\n\")\n",
    "\n",
    "    # Afisam atomii\n",
    "    # for atom in atomSet:\n",
    "    #     print(atom)\n",
    "\n",
    "# Apelarea funcției de parsare\n",
    "parsare('codePy.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcb1d14",
   "metadata": {},
   "source": [
    "## Partea 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2132e4ad",
   "metadata": {},
   "source": [
    "**4. Implementarea analizorului lexical**\n",
    "\n",
    "- Analizorul lexical acceptă la intrare un fișier text reprezentând un program sursă și întocmește ca date de ieșire tabelele:\n",
    "  - **FIP** - forma internă a programului sursă și\n",
    "  - **TS** - tabelul de simboluri.\n",
    "  \n",
    "- În plus, programul va trebui să semnaleze erorile lexicale și locul în care apar.\n",
    "\n",
    "Niște restricții suplimentare vor fi impuse analizoarelor lexicale.\n",
    "\n",
    "- Acestea se vor diferenția după următoarele criterii:\n",
    "  1. **Tabela de simboluri:**\n",
    "     - **a. Unică pentru identificatori și constante**  \n",
    "     - b. Separat pentru identificatori și constante\n",
    "     \n",
    "  2. **Organizarea tabelelor de simboluri:**\n",
    "     - **a. Tabel ordonat lexicografic**  \n",
    "     - b. Tabel arbore binar de căutare (ordine lexicografică)  \n",
    "     - c. Tabel de dispersie (hash)\n",
    "\n",
    "- Se cer, de asemenea:\n",
    "  - Implementarea structurilor de date cerute pentru tabela de simboluri.\n",
    "  - Organizarea **FIP** și **TS** trebuie aleasă astfel încât accesul la informațiile pentru un atom lexical în **TS** să se facă în **Θ(1)** pe baza informațiilor din **FIP**.\n",
    "  - Salvarea **FIP** și **TS** în fișiere. Poziția în **TS** (din fișierul corespunzător **FIP**) va fi numărul liniei din fișierul **TS** în care este stocată informația referitoare la acel atom lexical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3809f1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analiza lexicală în curs...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Definirea claselor pentru FIP și TS\n",
    "class FIP:\n",
    "    def __init__(self):\n",
    "        self.entries = []\n",
    "\n",
    "    def add(self, token, ts_index):\n",
    "        self.entries.append((token, ts_index))\n",
    "\n",
    "    def print(self):\n",
    "        # Afișare FIP în terminal\n",
    "        print(\"\\nForma Intermediară a Programului (FIP):\")\n",
    "        df = pd.DataFrame(self.entries, columns=['Token', 'TS_Index'])\n",
    "        print(tabulate(df, headers='keys', tablefmt='grid'))\n",
    "\n",
    "    def save(self, filename):\n",
    "        # Salvare FIP în format tabelar într-un fișier text\n",
    "        df = pd.DataFrame(self.entries, columns=['Token', 'TS_Index'])\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(tabulate(df, headers='keys', tablefmt='grid'))\n",
    "\n",
    "class TS:\n",
    "    def __init__(self):\n",
    "        self.symbols = []\n",
    "        self.symbol_dict = {}\n",
    "\n",
    "    def add(self, symbol):\n",
    "        # Implementare sortare prin inserție\n",
    "        if symbol not in self.symbol_dict:\n",
    "            self.symbols.append(symbol)\n",
    "            i = len(self.symbols) - 1\n",
    "            while i > 0 and self.symbols[i] < self.symbols[i - 1]:\n",
    "                self.symbols[i], self.symbols[i - 1] = self.symbols[i - 1], self.symbols[i]\n",
    "                i -= 1\n",
    "            self.symbol_dict = {sym: idx for idx, sym in enumerate(self.symbols)}\n",
    "        return self.symbol_dict[symbol]\n",
    "\n",
    "    def print(self):\n",
    "        # Afișare TS în terminal\n",
    "        print(\"\\nTabelul de Simboluri (TS):\")\n",
    "        data = [{'Symbol': sym, 'Number': idx} for sym, idx in self.symbol_dict.items()]\n",
    "        df = pd.DataFrame(data)\n",
    "        print(tabulate(df, headers='keys', tablefmt='grid'))\n",
    "\n",
    "    def save(self, filename):\n",
    "        # Salvare TS în format tabelar într-un fișier text\n",
    "        data = [{'Symbol': sym, 'Number': idx} for sym, idx in self.symbol_dict.items()]\n",
    "        df = pd.DataFrame(data)\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(tabulate(df, headers='keys', tablefmt='grid'))\n",
    "\n",
    "# Funcția de parsare și verificare lexicală\n",
    "def parsare_si_verificare(filename):\n",
    "    keywords = {'if', 'else', 'while', 'return', 'print', 'int(input())', 'input()'}\n",
    "    operators = {'+', '-', '*', '/', '=', '==', '!='}\n",
    "    delimiters = {'(', ')', '{', '}', ';'}\n",
    "\n",
    "    atomSet = []\n",
    "    with open(filename, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line_no, line in enumerate(lines, 1):\n",
    "        words = line.strip().split(\" \")\n",
    "        for word in words:\n",
    "            if word not in keywords and word not in operators and word not in delimiters:\n",
    "                if not re.match(r'^[a-zA-Z_]\\w*$', word) and not re.match(r'^\\d+(\\.\\d+)?$', word) and not re.match(r'^\\d+$', word):\n",
    "                    print(f\"Eroare lexicală la linia {line_no}: atom necunoscut '{word}'\")\n",
    "                    return None\n",
    "            atomSet.append((word, line_no))\n",
    "\n",
    "    return atomSet\n",
    "\n",
    "# Funcția principală de analiză lexicală\n",
    "def lexical_analyzer(source_file):\n",
    "    # Apelăm funcția de parsare și verificare lexicală\n",
    "    atomSet = parsare_si_verificare(source_file)\n",
    "    if atomSet is None:\n",
    "        print(\"Erori lexicale găsite. Parsarea și analiza lexicală au fost oprite.\")\n",
    "        return\n",
    "\n",
    "    ts = TS()\n",
    "    keywords = {'if', 'else', 'while', 'return', 'print', 'int(input())', 'input()'}\n",
    "    operators = {'+', '-', '*', '/', '=', '==', '!='}\n",
    "    delimiters = {'(', ')', '{', '}', ';'}\n",
    "\n",
    "    print(\"Analiza lexicală în curs...\")\n",
    "\n",
    "    # Prima trecere: Construirea TS\n",
    "    for token, line_no in atomSet:\n",
    "        if token not in keywords and token not in operators and token not in delimiters:\n",
    "            # Adaugam in TS doar atomii care sunt identificatori sau constante\n",
    "            if re.match(r'^[a-zA-Z_]\\w*$', token) or re.match(r'^\\d+(\\.\\d+)?$', token):\n",
    "                ts.add(token)\n",
    "            else:\n",
    "                print(f\"Eroare lexicală la linia {line_no}: atom necunoscut '{token}'\")\n",
    "\n",
    "\n",
    "    # A doua trecere: Construirea FIP\n",
    "    fip = FIP()\n",
    "    for token, line_no in atomSet:\n",
    "        if token in keywords or token in operators or token in delimiters:\n",
    "            fip.add(token, \"\")\n",
    "        elif re.match(r'^[a-zA-Z_]\\w*$', token):  # Identificator\n",
    "            ts_index = ts.symbol_dict[token]\n",
    "            fip.add('ID', ts_index)\n",
    "        elif re.match(r'^\\d+(\\.\\d+)?$', token):  # Constantă (inclusiv zecimală)\n",
    "            ts_index = ts.symbol_dict[token]\n",
    "            fip.add('CONST', ts_index)\n",
    "        else:\n",
    "            print(f\"Eroare lexicală la linia {line_no}: atom necunoscut '{token}'\")\n",
    "\n",
    "\n",
    "    # Salvare în fișiere TXT sub formă de tabel\n",
    "    fip.save('FIP.txt')\n",
    "    ts.save('TS.txt')\n",
    "\n",
    "# Apelarea funcției de analiză lexicală\n",
    "lexical_analyzer('codePy.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b16470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
